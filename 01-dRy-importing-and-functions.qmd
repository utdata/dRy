---
title: "Don't Repeat Yourself in R"
format: html
---

By Ella Barnes and Karina Kumar - Data Fellows of the Media Innovation Group at University of Texas at Austin.

## Goals of this session
- Import multiple files at once
- Create your own function
- Generate parameterized reports quickly

Why dRy? Data analysis often requires repetitive processes. These tools can help you maximize efficiency and stop repeating the same code over and over again. 

## The data

We will be working with two datasets: one about Taylor Swift songs and one about event-correlated power outages across the United States. 

The Taylor data came from [Kaggle](https://www.kaggle.com/code/jarredpriester/taylor-swift-spotify-notebook). 

The power outages data came from [data.gov](https://catalog.data.gov/dataset/event-correlated-outage-dataset-in-america).

Since this lesson is not about cleaning or analysis, we have already done those things for you. But we will be importing the data as if it was in multiple files (which wouldn't be the case after cleaning usually).

## How this lesson works

For each part of the lesson, we will provide an example using the Taylor Swift dataset. Then we will give you an opportunity to try the concept on your own (OYO) using the power outages dataset. Most code chunks are numbered and correlate to number markdown lists afterwards. You can also view this lesson as a Quarto site if you want to come back at a later date to review what we do today. 

# Importing multiple files at once

First, we are going to look at how to import multiple files at once using the map() function. The Taylor Swift song data is divided into one csv for each album, and we want to bring it all into one object. 

## Set up

Importing our needed libraries. We use tidyverse for almost everything.

```{r}

#| label: setup   #<1>
#| message: false #<2>
#| warning: false #<3>

library(tidyverse)
library(janitor)

```

1. This sets the chunk label. Using `setup` specifically will run this code chunk first every time you render or run the page. 
2. `#| message: false`  hides the output messages that usually appear when you import libraries. We will show it again later.
3. Sometimes you do something that creates a warning in R, but you don't always care. `#| warning: false` hides those warnings. We will talk about this more later too. 

## Creating a list of file names

```{r}
#| label: import

taylor_files_list <- list.files(  #<1>
  "data-processed-taylor",        #<2>
  pattern = ".csv",               #<3>
  full.names = TRUE               #<4>   
  # Try changing full.names to FALSE and see what happens to the output
  )

taylor_files_list
```

1. `list.files()` creates a character vector of the names of files
2. The first argument is the path to find the files. In this case, we want to go into the directory (folder) called `data-proccessed-taylor`.
3. `pattern` is an optional argument that takes a regular expression to define which specific files you want. In this case we want all files that contain `.csv`.
4. `full.names = TRUE` gets the full path name rather than just the files name. Let's change this and see what happens...

## Reading in and combining the files

```{r}
#| label: combine-files 

taylor_songs <- taylor_files_list |>  #set_names(basename) |> 
  map(                                        #<1>
  read_csv,                                   #<2>
  col_types = cols(album = col_character())   #<3>    
) |> list_rbind() |>                          #<4>
  clean_names()                               #<5>

taylor_songs |> head(15)

# comment out the above line and the comma to see what happens      b. #<6>
# uncomment set_names(basename) and add names_to = "source" inside list_rbind() #<7>
```

1. First we create a new object called `talyor_songs` which is where all our data will end up. Then we take our list of files names `taylor_files_list` and put it in the `map()` function. `map()` applies a method (in this case `read_csv()`) to every item in a list and returns a list as a result.
2. Our first argument is the function we want to apply which is `read_csv()` which returns a dataframe of the csv. 
3. After the comma is `col_types =` which is an argument in `read_csv()`. This argument can set any or all of the column types as you import the data.  
4. Once we use map to create a list of our newly created object from read_csv, we want to combine them all into one single object which already called `taylor_songs`, so we use `list_rbind()`. 
5. And then just to make everything look nice and uniform we use `clean_names()` to change the column names. 
6. What happens when you comment out the `col_types = ` line? Taylor's album 1989 shows up as a number while the other album names show up as strings so get an error and can't join the columns when we use list_rbind().
7. Let's add `set_names(basename)` by uncommenting it and add `names_to = "source"` inside `list_rbind()`. Look at the output. It creates a column that shows which file each row came from. This can be really helpful when you maybe don't have a column that has the date or year, but the file name does contain it. You can then extract it from that column.
8. This 

# OYO: Importing multiple files at once - Power Outages

Now, try this on your own with the power outage data.

Instructions: The files you want to import are all in a folder called `data-processed-power`. We have already given you the object names to put your code in to help with consistency and preventing repetition across files. Uncomment each of the lines and input code to read in the csv's. 

```{r}
#| label: import-oyo

#power_files_list <- 

```

```{r}
#| label: combine-files-oyo

#power_outages <-  

#power_outages |> glimpse()
```

# Creating your own function

We want to create a function that creates a graph we can change over and over again.

First, let's take a look at the Taylor Swift song data...

```{r}
taylor_songs |> head(50)
```

There are a lot of columns that describe different aspects of the music, but two that are obvious and stand our are popularity and song length. Do Taylor's songs have any sort of correlation in popularity to length of the song? Let's graph it to find out. The data creator has told us that popularity is on a scale of 1-100. 

```{r}
ggplot(taylor_songs, aes( 
  x = duration_min,
  y = popularity)) + 
  geom_point() 
```
*For our Taylor Swift fans, can you tell which song is all the way on the right?*

What if we want to add labels and scale the axes differently? And what if we wanted to make the same graph for multiple albums? That would require a lot of recoding things we've already typed out before (or a lot of copy and pasting). Rather than rewriting this the same methods over and over again, we can make a function that does it for us. 

We are going to call our function `graphing_taylor`.

```{r}
graphing_taylor <- function(song_data) {                       #<1>  
  ggplot(song_data, aes(x= duration_min, y = popularity)) +
    geom_point() +
    labs(x = "Length of song in minutes", y = "Popularity") +
    scale_x_continuous(n.breaks = 10) +
    scale_y_continuous(limits = c(0,100))}

```

1. Here we use the `function` method with one argument: `song_data` and then we define the function using the curly braces `{}`. This is extremely similar to JavaScript. 

Now that we've built our function we can apply it to whatever album(s) we want. So let's look at all the versions of the Red album first. 

```{r}
red <- taylor_songs |> filter(album |> str_detect("Red"))

red
                      
```

Now we can apply our graphing function to the `red` object. 

```{r}
graphing_taylor(red)
```

*And again we see our "All Too Well (10 minute version)" as a big outlier.*

Now try it on your own.

# OYO: Creating your own function

Let's look at the power outage data.

```{r}
#power_outages |> head(50)
```

We want to see what the top 10 causes of power outages are for each state and how many outages there have been for each category.
To do this for Texas, we would filter for our state,  group_by the event_category, count the number of occurances for each category, arrange it in descending order and then take the top ten. 

```{r}
filtered_data <- power_all |> filter(state_event |> str_detect("Texas")) |>
  group_by (event_category) |> 
  summarize(total_events = n()) |> 
  arrange(total_events |> desc()) |> 
  head(10)
```

Now we want to create a bar chart. 

```{r}
ggplot(filtered_data, 
         aes(x = total_events, 
             y = reorder(event_category, total_events))) +
         geom_col() +
         labs(x = "Occurances", y = "Outage Cause")  +
        scale_x_continuous(labels = comma)
```

Now, your turn. We want to put these two steps into a single function so we can do it all at once for any state we want. Try it below and test if it matches the graph output above for Texas. You can copy and paste the filtering and graphing code but will need to adjust a few things for the funciton to work. 

```{r}
#| label: function

filter_graph_function <- function (state) {
  
}

```

Test it with Texas. 

```{r}
filter_graph_function("Texas")
```

Try it with your home state or another state if you're from Texas too.  

```{r}
filter_graph_function("YOUR STATE HERE")
```


